{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Dense, Input, Dropout, Activation, Conv1D,Flatten\n",
    "from keras.layers import MaxPooling1D,GlobalMaxPool1D\n",
    "from keras.models import Sequential,Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BankChurners.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Attrition_Flag = df.Attrition_Flag.replace({'Attrited Customer':1,'Existing Customer':0})\n",
    "df.Gender = df.Gender.replace({'F':1,'M':0})\n",
    "df = pd.concat([df,pd.get_dummies(df['Education_Level']).drop(columns=['Unknown'])],axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['Income_Category']).drop(columns=['Unknown'])],axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['Marital_Status']).drop(columns=['Unknown'])],axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['Card_Category']).drop(columns=['Platinum'])],axis=1)\n",
    "df.drop(columns = ['Education_Level','Income_Category','Marital_Status','Card_Category','CLIENTNUM'],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder = \"OneHotEncoder\"\n",
    "Encoder = \"Label Encoder\"\n",
    "# OverSamplingTecnique = \"\"\n",
    "OverSamplingTecnique = \"SMOTE-Enn\"\n",
    "#OverSamplingTecnique = \"SMOTE-Tomek\"\n",
    "#OverSamplingTecnique = \"SMOTE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Encoder == \"Label Encoder\":\n",
    "  print(\"Applying Label Encoder\")\n",
    "  df_final = df.copy()\n",
    "  le = LabelEncoder()\n",
    "  text_data_features = ['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count',\n",
    "                     'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',\n",
    "                     'Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct',\n",
    "                     'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\n",
    "\n",
    "  print('Label Encoder Transformation')\n",
    "  for i in text_data_features :\n",
    "      df_final[i] = le.fit_transform(df_final[i])\n",
    "      print(i,' : ',df_final[i].unique(),' = ',le.inverse_transform(df_final[i].unique()))\n",
    "\n",
    "\n",
    "X = df_final.drop(['Attrition_Flag'], axis=1).copy()\n",
    "Y = df_final['Attrition_Flag'].copy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X)\n",
    "X=X_resampled_scaled\n",
    "Y=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "font_size = 20\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.labelsize'] = font_size\n",
    "plt.rcParams['axes.titlesize'] = font_size + 2\n",
    "plt.rcParams['xtick.labelsize'] = font_size - 2\n",
    "plt.rcParams['ytick.labelsize'] = font_size - 2\n",
    "plt.rcParams['legend.fontsize'] = font_size - 2\n",
    "\n",
    "colors = ['#00A5E0', '#DD403A']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "sns.countplot(x='Attrition_Flag', data=df_final, palette=colors, ax=ax)\n",
    "\n",
    "for index, value in enumerate(df_final['Attrition_Flag'].value_counts()):\n",
    "    label = '{}%'.format(round((value / df_final['Attrition_Flag'].shape[0]) * 100, 2))\n",
    "    ax.annotate(label,\n",
    "                xy=(index, value + 250),\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                color=colors[index], weight='bold',\n",
    "                size=font_size + 4)\n",
    "\n",
    "ax.set_xticklabels(['Retained', 'Churned'], fontweight='bold')\n",
    "ax.set_xlabel('Status', fontweight='bold')\n",
    "ax.set_ylabel('Count', fontweight='bold')\n",
    "ax.set_ylim([0, 10000]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OverSamplingTecnique == \"SMOTE\":\n",
    "  print(\"Applying SMOTE\")\n",
    "  smote = SMOTE()\n",
    "\n",
    "  X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
    "  scaler = StandardScaler()\n",
    "  X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "  X=X_resampled_scaled\n",
    "  Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OverSamplingTecnique == \"SMOTE-Tomek\":\n",
    "  print(\"Applying SMOTE-Tomek\")\n",
    "\n",
    "  smote_tomek = SMOTETomek()\n",
    "  X_resampled, y_resampled = smote_tomek.fit_resample(X, Y)\n",
    "  scaler = StandardScaler()\n",
    "  X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "  X=X_resampled_scaled\n",
    "  Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OverSamplingTecnique == \"SMOTE-Enn\":\n",
    "  print(\"Applying SMOTE-Enn\")\n",
    "\n",
    "  smote_enn = SMOTEENN()\n",
    "  X_resampled, y_resampled = smote_enn.fit_resample(X, Y)\n",
    "  scaler = StandardScaler()\n",
    "  X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "  X=X_resampled_scaled\n",
    "  Y=y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "font_size = 20\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['axes.linewidth'] = 1\n",
    "plt.rcParams['axes.labelsize'] = font_size\n",
    "plt.rcParams['axes.titlesize'] = font_size + 2\n",
    "plt.rcParams['xtick.labelsize'] = font_size - 2\n",
    "plt.rcParams['ytick.labelsize'] = font_size - 2\n",
    "plt.rcParams['legend.fontsize'] = font_size - 2\n",
    "\n",
    "colors = ['#00A5E0', '#DD403A']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "X_resampled_scaled_df = pd.DataFrame(X_resampled_scaled)\n",
    "X_resampled_scaled_df['Attrition_Flag'] = y_resampled\n",
    "\n",
    "sns.countplot(x='Attrition_Flag', data=X_resampled_scaled_df, palette=colors, ax=ax)\n",
    "\n",
    "for index, value in enumerate(y_resampled.value_counts()):\n",
    "    label = '{}%'.format(round((value / y_resampled.shape[0]) * 100, 2))\n",
    "    ax.annotate(label,\n",
    "                xy=(index, value + 250),\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                color=colors[index], weight='bold',\n",
    "                size=font_size + 4)\n",
    "\n",
    "ax.set_xticklabels(['Retained', 'Churned'], fontweight='bold')\n",
    "ax.set_xlabel('Status', fontweight='bold')\n",
    "ax.set_ylabel('Count', fontweight='bold')\n",
    "ax.set_ylim([0, 10000]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor_from_dl_model(model, data_seq, layer_num):\n",
    "    \"\"\"\n",
    "    Create a new representation of the data by extracting the output of the given DL layer_num\n",
    "    Parameters\n",
    "    ----------\n",
    "    @param model: DL model\n",
    "    @param data_seq: DNA sequences for extracting DL representation\n",
    "    @param layer_num: the output of this layer is used as feature representation\n",
    "    \"\"\"\n",
    "    # Get the input tensor of the model\n",
    "    input_tensor = model.layers[0].input\n",
    "    # Create a new model that only includes the desired layer\n",
    "    new_model = tf.keras.models.Model(inputs=input_tensor, outputs=model.layers[layer_num].output)\n",
    "    layer_outs = new_model.predict(data_seq)\n",
    "    out_array = np.array(layer_outs)\n",
    "    out_array = out_array.flatten()\n",
    "    out_array = np.reshape(out_array, (data_seq.shape[0], -1))\n",
    "    return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,Flatten,GRU,SimpleRNN,Concatenate\n",
    "\n",
    "def kfold(Model_No):\n",
    "  print(\"Applying K-fold\")\n",
    "\n",
    "\n",
    "  # Assuming X and Y are your input and target data\n",
    "  # Define the number of folds\n",
    "  num_folds = 2\n",
    "\n",
    "  # Initialize lists to store the evaluation results\n",
    "  accuracy_scores = []\n",
    "  precision_scores = []\n",
    "  recall_scores = []\n",
    "  f1_scores = []\n",
    "\n",
    "  # Perform stratified k-fold cross-validation\n",
    "  fold_number = 1  # Initialize the fold number\n",
    "  skf = StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
    "  for train_index, test_index in skf.split(X, Y):\n",
    "      print(f\"Fold {fold_number}/{num_folds}:\")\n",
    "      # Split the data into training and test sets for the current fold\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      # X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    #  if Model_No==1: # LogisticRegression\n",
    "    #    model = LogisticRegression(random_state=random_state)\n",
    "\n",
    "\n",
    "   #     model.fit(X_train, Y_train)\n",
    "\n",
    "   #   elif Model_No==2: # SVC\n",
    "    #    model = SVC(random_state=random_state)\n",
    "\n",
    " #       model.fit(X_train, Y_train)\n",
    "   #   elif Model_No==3: # RandomForestClassifier\n",
    "    #   model = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "      #  model.fit(X_train, Y_train)\n",
    "      if Model_No==1:\n",
    "        model=Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "        model.add(Conv1D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=32,kernel_size=3,padding='same',activation='relu'))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(64, activation='tanh'))\n",
    "        model.add(Dense(32, activation='tanh'))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile and train the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train, epochs=30, batch_size=32, verbose=1,validation_split=0.2)\n",
    "      elif Model_No==2:\n",
    "        model=Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "        model.add(Conv1D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=32,kernel_size=3,padding='same',activation='relu'))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dense(64, activation='tanh'))\n",
    "        model.add(Dense(32, activation='tanh'))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile and train the model\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "        model.compile(optimizer= optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train, epochs=3, batch_size=32, verbose=1,validation_split=0.2)\n",
    "        out_array=feature_extractor_from_dl_model(model, X_train, -6)\n",
    "        model2 = RandomForestClassifier(random_state=random_state)\n",
    "        model2.fit(out_array, Y_train)\n",
    "        out_array=feature_extractor_from_dl_model(model, X_test, -6)\n",
    "        X_test=out_array\n",
    "        model=model2\n",
    "      elif Model_No==3:\n",
    "        model=Sequential()\n",
    "        model.add(Input(shape=(X_train.shape[1], 1)))\n",
    "        model.add(LSTM(128, return_sequences=True))\n",
    "        model.add(LSTM(16))\n",
    "\n",
    "     # Compile and train the model\n",
    "      # optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='ADAM', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        history=model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1,validation_split=0.2)\n",
    "      Y_pred = model.predict(X_test)\n",
    "      Y_pred_binary = np.round(Y_pred).flatten()\n",
    "\n",
    "      # Calculate evaluation metrics\n",
    "      accuracy = accuracy_score(Y_test, Y_pred_binary)\n",
    "      precision = precision_score(Y_test, Y_pred_binary)\n",
    "      recall = recall_score(Y_test, Y_pred_binary)\n",
    "      f1 = f1_score(Y_test, Y_pred_binary)\n",
    "\n",
    "      print(' Test--------- | accuracy:{:<6.4f} | precision:{:<6.4f} |recall:{:<6.4f} | F1:{:<6.4f}'.format( accuracy, precision, recall, f1))\n",
    "      # Append the scores to the respective lists\n",
    "      accuracy_scores.append(accuracy)\n",
    "      precision_scores.append(precision)\n",
    "      recall_scores.append(recall)\n",
    "      f1_scores.append(f1)\n",
    "      fold_number += 1\n",
    "\n",
    "\n",
    "  # Calculate the average scores\n",
    "  avg_accuracy = np.mean(accuracy_scores)\n",
    "  avg_precision = np.mean(precision_scores)\n",
    "  avg_recall = np.mean(recall_scores)\n",
    "  avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "  # Print the average scores\n",
    "  print(avg_accuracy)\n",
    "  print(avg_precision)\n",
    "  print(avg_recall)\n",
    "  print(avg_f1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
